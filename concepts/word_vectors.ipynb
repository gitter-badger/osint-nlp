{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computers only know how to deal with numbers.**\n",
    "\n",
    "We can represent images as numbers too... but how do they deal with text?\n",
    "\n",
    "A computer can tell you whether two strings are different, but how can it understand that `football` and `Ronaldo` are related to `Messi`, or that `Apple` in `Apple is not an orange` is not the company?\n",
    "\n",
    "We need to create a numerical representation for our natural language that captures the meaning of words, their semantic relationships, and the different types of contexts they are used in. This numerical representation of text is called `word embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"Georgetown is a great university, nay a fantastic university!\"\n",
    "\n",
    "# Number of words = 9\n",
    "# Vocabulary = [\"a\", \"fantastic\", \"Georgetown\", \"great\", \n",
    "#               \"is\", \"nay\", \"university\"]\n",
    "# Length of vocabulary = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A vector representation of a word may be a `one-hot encoded` vector\n",
    "\n",
    "one_hot = {\n",
    "    \"a\":          [1, 0, 0, 0, 0, 0, 0], \n",
    "    \"fantastic\":  [0, 1, 0, 0, 0, 0, 0], \n",
    "    \"Georgetown\": [0, 0, 1, 0, 0, 0, 0], \n",
    "    \"great\":      [0, 0, 0, 1, 0, 0, 0], \n",
    "    \"is\":         [0, 0, 0, 0, 1, 0, 0], \n",
    "    \"nay\":        [0, 0, 0, 0, 0, 1, 0], \n",
    "    \"university\": [0, 0, 0, 0, 0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consider a corpus of D documents and N unique tokens. \n",
    "# The size of the Count Matrix is by D x N. \n",
    "# Each row (i) in the matrix contains the frequency of tokens in document D(i).\n",
    "\n",
    "d1 = \"Georgetown is a great university, nay is a fantastic university!\"\n",
    "d2 = \"The best university is Trump University\"\n",
    "vocabulary = [\"a\", \"best\", \"fantastic\", \"Georgetown\", \"great\", \n",
    "              \"is\", \"nay\", \"the\", \"Trump\", \"university\"]\n",
    "\n",
    "# Here D=2, N=10\n",
    "\n",
    "CV = [\n",
    "    [2, 0, 1, 1, 1, 2, 1, 0, 0, 2],\n",
    "    [0, 1, 0, 0, 0, 1, 0, 1, 1, 2],\n",
    "]\n",
    "\n",
    "# The sum of each row should be equal to the number of words in the document\n",
    "# The sum of each column should be equal to the number of times the word\n",
    "# appears in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be quite a few variations while preparing the above matrix:\n",
    "- In real world applications we might have a corpus with millions of unique words. The matrix above will be a very sparse one. An alternative is to consider only the most frequent 10,000 words.\n",
    "- We can either take the frequency (number of times a word appears in the document) or the presence (does the word appear in the document?) to be the entries in the Count Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common words like `is`, `the`, `a` etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document on `Georgetown` is going to contain more occurences of the word `Georgetown` in comparison to other documents, while common words are going to have a high frequency in every document.\n",
    "\n",
    "`TF-IDF` penalizes common words, and gives more importance to rare words that appear in a subset of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TF = (Number of times term t appears in a document)/(Number of terms in the document)`\n",
    "\n",
    "e.g. `TF(\"university\", d1) = 0.2`\n",
    "\n",
    "It quantifies the contribution of that specific word to the document: words relevant to the document should be frequent.\n",
    "\n",
    "`IDF = log(N/n)`, where `N` is the number of documents, and `n` is the number of documents a term t has appeared in.\n",
    "\n",
    "e.g. `IDF(This) = log(2/2) = 0`\n",
    "\n",
    "If a word appears in every document, probably it's not relevant to that particular document.\n",
    "\n",
    "Let us compute IDF for the word ‘Messi’.\n",
    "\n",
    "IDF(Messi) = log(2/1) = 0.301.\n",
    "\n",
    "Now, let us compare the TF-IDF for a common word ‘This’ and a word ‘Messi’ which seems to be of relevance to Document 1.\n",
    "\n",
    "TF-IDF(This,Document1) = (1/8) * (0) = 0\n",
    "\n",
    "TF-IDF(This, Document2) = (1/5) * (0) = 0\n",
    "\n",
    "TF-IDF(Messi, Document1) = (4/8)*0.301 = 0.15\n",
    "\n",
    "As, you can see for Document1 , TF-IDF method heavily penalises the word ‘This’ but assigns greater weight to ‘Messi’. So, this may be understood as ‘Messi’ is an important word for Document1 from the context of the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was about to talk about `word2vec`, then Google last month published their\n",
    "[sentence encoder](https://arxiv.org/pdf/1803.11175.pdf). Embedding the whole sentence instead of single words works much better, so that's what we'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
